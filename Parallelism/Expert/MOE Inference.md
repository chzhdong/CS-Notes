# 混合专家模型推理优化策略

本文主要基于 **A Survey on Inference Optimization Techniques for Mixture of Experts Models** 综述论文对目前大模型领域主流的混合专家架构的推理优化策略进行总结和梳理，以便于进一步的研究和学习。

## 论文概述

混合专家模型，即 Mixture of Experts 架构在 90 年代就被提出。随着大模型领域的不断发展，为了提高模型性能，即增加模型的参数量，而不去增加模型推理的计算量，混合专家模型在大模型领域中被广泛地使用。

混合专家模型的核心原则就是将模型的性能分发到多个专用的子网络，即专家，并通过一个可学习的路由机制基于当前输入去选择性地激活相关的专家。目前主流的 MOE 架构模型包括 Mixtral、Qwen、DeepSeek、Llama。

基于目前的 MOE 的推理流程，论文将其划分为三个层次：模型层、系统层、硬件层。对于模型层而言，其主要的挑战在于设计有效的专家架构以平衡模型性能和计算效率，同时路由策略需要优化专家选择和负载均衡。系统层主要关注分布式计算中调度问题以及专家的放置和激活。硬件层主要解决传统硬件架构对于密集计算和稀疏计算的不匹配，以及管理显存带宽和动态的专家切换问题。

## 混合专家模型

对于 MOE 混合架构而言，存在一个路由网络 $R(x)$  以及一组专家网络 $E_1, E_2, ..., E_N$，其中 N 表示全部专家的数量。MOE 架构基础的计算原则可以表示为 $y = \sum_{i=1}^{N} g_i(x) \cdot E_i(x)$，其中 $g_i(x)$ 代表专家 i 的门函数，$E_i(x)$ 表示专家 i 的输出。

对于传统的 MOE 架构，首先通过路由器计算专家选择概率：$\theta = Softmax(R(x))$，其次通过已计算的概率选择出 K 个专家 $E_{selected} = TopK(\theta, K)$，最后通过加权计算每个专家的输出获得最终结果：$y = \sum_{i \in E_{selected} \frac{\theta_i}{\sum_{j \in E_{selected}} \theta_j}} \cdot y_i$

## 模型层

MOE-based Attention、MOE-based FFN，由于工作不会涉及架构修改，所以不做过多叙述。

专家剪枝一般是将重要性较低的专家或者融合属于同一组别的专家；专家量化目前的主要方法是根据专家的重要性不同，选择不同的量化等级，以实现较小的性能损失提高计算效率；专家蒸馏则一般是使用 MOE 架构的模型指导密集架构的大模型进行蒸馏；专家分解则一般是指用类似于 LORA 的手段对矩阵计算进行低秩分解，以降低模型的权重参数。

动态路由是通过充分利用 MOE 架构的稀疏性，动态的减少对于输入所激活的专家数量以提升计算效率；稀疏到密集一般是将稀疏的 MOE 架构转化为密集计算的模型，一般是通过知识蒸馏等方式进行实现。

## 系统层

MOE 模型部署的两个场景：云环境及边缘环境。其中，云环境即为服务器集群，可以允许将 MOE 分布式部署在集群服务器中进行专家并行；而边缘环境的单个设备无法直接加载 MOE 的全部参数，需要卸载部分参数到主存甚至硬盘中存储。

专家并行中，除专家外的模型参数在所有 GPU 上是一致的，而不同专家分配在不同的 GPU 上运行。通过 ALL-to-ALL 通信来实现专家的激活以及获取对应专家的输出结果。基于上述流程，主要瓶颈存在于计算以及通信的开销，目前主要有四种解决方法。

并行策略设计，通过结合多种并行策略设计，而不仅仅依靠专家并行来提升推理效率；负载均衡，是为了解决由于部分专家激活显著要与其他专家的问题，通过设计辅助的负载均衡损失函数训练路由器，或实现使用哈希函数的路由模型来阻止不均衡情况。进阶的方法通常关注优化专家的放置策略；通信优化，诸多工作主要关注于分等级的通信策略或者数据压缩技术来减少开销；任务调度，通过重叠通信和计算任务来减少端到端的运行时间，或利用架构或者算法的创新来实现高效的重叠。

专家卸载方面不再过多赘述。

## 硬件层

目前的硬件层对于 MOE 推理所解决的关键挑战通过创新的架构或者软硬件协同设计方法，这些优化方法主要是提升计算效率、异构计算单元以及存储访问模式。