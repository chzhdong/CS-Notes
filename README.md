# Large Model

## Papers

### Model Architectrue

- Attention is All you need (Transformer)

### Attention Acceleration

- Efficient Memory Management for Large Language Model Serving with PagedAttention
- FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

### Mixture of Experts

#### Offloading

- HOBBIT: A Mixed Precision Expert Offloading  System for Fast MoE Inference
- Fidder: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
- MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs
- MOE-INFINITY: Efficient MoE Inference on Personal Machines  with Sparsity-Aware Expert Cache
- fMOE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving

### Quantization

- LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale
- GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers
- AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

### Batching

#### Request Batching

- Orca: A Distributed Serving System for Transformer-Based Generative Models
- Fairness in Serving Large Language Models
- Llumnix: Dynamic Scheduling for Large Language Model Serving

## Blogs

- [LLM推理算法简述](https://zhuanlan.zhihu.com/p/685794495)

- [从MHA、MQA、GQA到MLA](https://kexue.fm/archives/10091)

# C++

## Websites

- [cplusplus](https://cplusplus.com/)

- [cppreference](https://en.cppreference.com/w/)

## Books

- [Modern C++](https://changkun.de/modern-cpp/)

- [Modern CMake](https://cliutils.gitlab.io/modern-cmake/README.html)

# CUDA

## Kernels

- [Kernels](./CUDA/Kernels)

## Blogs

- [谭升的博客](https://face2ai.com/)
